# 第一章 大模型简介

## 1.概述

### 	1.发展历程

语言建模的研究始于20世纪90年代，但那时采用的统计学习方法不能满足理解复杂语言规则的需求。

在2003年，Bengio首次将深度学习的思想融入语言模型中，2018年左右，Transformer架构的神经网络模型被引入，自此，大语言模型的时代到来。

### 	2.大语言模型的概念

**大语言模型（英文：Large Language Model，缩写LLM）是一种人工智能模型，皆在理解和生成人类语言。**

大语言模型包含数百亿（或更多）参数的语言模型，然后通过大量文本数据训练就可以更好理解人类语言并且作出答复。

现在大语言模型的极限已达万亿参数（GPT-4），这其中一个杰出应用就是**ChatGPT**，它的出现可以说是又一次科技革命。

​	**LLM的应用与影响**

LLM的出现不仅影响了**自然语言处理**领域，而且还带动了**信息检索**领域和**计算机视觉**领域。

这两者也可以通过类似于LLM的大数据训练和多参数来使其更能理解人类的需求。在未来，这种训练人工智能的思路会应用到各个领域。

## 2.大模型的能力、特点

### 1.大模型的能力

1.1**涌现能力**

涌现能力就是我们常说的量变引起质变，随着参数越来越多和训练数据的增加，LLM的性能也提升了许多。

下面是三个典型的LLM涌现能力：

- 上下文学习：可以理解上下文并给出答复。
- 指令遵循：也就是指令微调，可以在指令形式化描述的时候执行未见过的任务，且表现良好。
- 逐步推理：采用“思维链”，可以利用包含中间推理步骤的提示完成任务。

1.2**作为基座模型支持多元应用**

在2021年，斯坦福大学等多所高校的研究人员提出了**基座模型（foundation model）**的概念，这更清晰地描述了之前学界所称的预训练模型的作用。这是一种全新的AI技术范式，借助于海量无标注数据的训练，获得可以适用于大量下游任务的大模型（单模态或者多模态）。这样，多个应用可以只依赖于一个或少数几个大模型进行统一建设。

1.3**支持对话作为统一入口的能力**

### 2.大模型的特点

1. 巨大规模
2. 预训练和微调
3. 上下文感知
4. 多语言支持
5. 多模态支持
6. 涌现能力
7. 多领域应用
8. 伦理和风险问题

## 3.常见大模型

1. ChatGPT
2. GPT-4
3. Claude系列
4. PaLM系列
5. 文心一言
6. 讯飞星火
7. LLaMA系列
8. GLM系列
9. 通义千问
10. Baichuan系列

7-10是开源的LLM大模型。

# 4.LangChain

**LangChain 框架是一个开源工具，充分利用了大型语言模型的强大能力，以便开发各种下游应用。它的目标是为各种大型语言模型应用提供通用接口，从而简化应用程序的开发流程**。

**LangChain核心组件**

LangChian 作为一个大语言模型开发框架，可以将 LLM 模型（对话模型、embedding模型等）、向量数据库、交互层 Prompt、外部知识、外部代理工具整合到一起，进而可以自由构建 LLM 应用。 LangChain 主要由以下 6 个核心模块组成:

- **模型输入/输出（Model I/O）**：与语言模型交互的接口
- **数据连接（Data connection）**：与特定应用程序的数据进行交互的接口
- **链（Chains）**：将组件组合实现端到端应用。
- **记忆（Memory）**：用于链的多次运行之间持久化应用程序状态；
- **代理（Agents）**：扩展模型的推理能力。用于复杂的应用的调用序列；
- **回调（Callbacks）**：扩展模型的推理能力。用于复杂的应用的调用序列；